"""
AplicaciÃ³n Streamlit para descargar datos desde SQL Server a Google Drive
Adaptado para pipeline ETL con Google Drive Desktop

ğŸ“‚ Guarda automÃ¡ticamente en Google Drive para posterior procesamiento con Snowflake
âœ… Incluye descarga de 11 tablas base con columna EAN en movGC_vtDocumentoVtaDet

Autor: Sistema  
Fecha: 2025-01-22
"""

import streamlit as st
import pandas as pd
import pyodbc
from datetime import datetime, timedelta
import os
from pathlib import Path
from dotenv import load_dotenv

# Cargar configuraciÃ³n
env_path = Path(__file__).parent / ".env"
load_dotenv(env_path)

st.set_page_config(
    page_title="Descarga SQL Server â†’ Drive",
    page_icon="ğŸ“¥",
    layout="wide"
)

# ==========================================================================
# CONFIGURACIÃ“N
# ==========================================================================

DRIVE_BASE_DIR = os.getenv("DRIVE_BASE_DIR", r"G:\Mi unidad\ETL_Snowflake")
PAISES_STR = os.getenv("PAISES_FOLDERS", "CHILE,COLOMBIA,ECUADOR,PERU")
PAISES = [p.strip() for p in PAISES_STR.split(",") if p.strip()]

# Fallback a local
if not Path(DRIVE_BASE_DIR).exists():
    st.warning(f"âš ï¸ Google Drive no detectado en: {DRIVE_BASE_DIR}")
    st.info("ğŸ’¡ Usando carpeta local temporal. Instala Google Drive Desktop para sincronizaciÃ³n automÃ¡tica.")
    DRIVE_BASE_DIR = r"C:\Ciencia de Datos\Proceso_Snowflake\temp_data"
    Path(DRIVE_BASE_DIR).mkdir(parents=True, exist_ok=True)

BASE_DIR = Path(DRIVE_BASE_DIR)

# Crear carpetas por paÃ­s
for pais in PAISES:
    (BASE_DIR / pais).mkdir(parents=True, exist_ok=True)

# ConfiguraciÃ³n SQL Server
SERVERS_CONFIG = {
    'CHILE': {
        'server': r'IBMSQLN1\DynamicsChile',
        'database': 'GPCPR',
        'user': 'rdgp',
        'password': 'P3muGP@386x'
    },
    'COLOMBIA': {
        'server': r'IBMSQLN1\DynamicsColombia',
        'database': 'GPCOP',
        'user': 'rdgp',
        'password': 'P3muGP@386x'
    },
    'ECUADOR': {
        'server': r'IBMSQLN1\DynamicsEcuador',
        'database': 'GPECP',
        'user': 'rdgp',
        'password': 'P3muGP@386x'
    },
    'PERU': {
        'server': r'IBMSQLN1\DynamicsPeru',
        'database': 'GPPER',
        'user': 'rdgp',
        'password': 'P3ruGP@386x'
    }
}

DRIVER = 'ODBC Driver 18 for SQL Server'

# Tablas base a descargar
TABLAS_BASE = [
    "movGC_DocumentoxDistribucion",
    "movGC_vtDocumentoVtaCab",
    "movGC_vtDocumentoVtaDet",
    "maeGC_ProductoEquiv",
    "maeGC_cfEstado",
    "maeGC_cfTipoDocumento",
    "RM00101",
    "RM00201",
    "maeGC_cfConcepto",
    "maeGC_Producto",
    "maeGC_Marca"
]

# SPs disponibles
SPS_DISPONIBLES = {
    'ğŸ“Š Reporte Ãšnico de Ventas': {
        'sp': 'uspGC_RptReporteUnicoDeVentasMACROS',
        'params': ['fecha_inicio', 'fecha_fin'],
        'desc': 'Documento de ventas completo',
        'agregar_ean': True
    },
    'ğŸ“ˆ Reporte Ventas Sellin': {
        'sp': 'uspGC_RptReporteUnicoDeVentasSellinMACROS',
        'params': ['fecha_inicio', 'fecha_fin'],
        'desc': 'Ventas sell-in con detalle',
        'agregar_ean': True
    },
    'ğŸª Reporte Ventas Mercado': {
        'sp': 'uspGC_RptReporteUnicoDeVentasMercadoMACROS',
        'params': ['fecha_inicio', 'fecha_fin'],
        'desc': 'Ventas por mercado',
        'agregar_ean': True
    },
    'ğŸ‘¥ Listar Clientes': {
        'sp': 'uspGC_ListarClientesMACROS',
        'params': [],
        'desc': 'Listado completo de clientes',
        'agregar_ean': False
    },
    'ğŸ“¦ Listar Productos Detallado': {
        'sp': 'uspGC_ListarProductoDetalladoMACROS',
        'params': [],
        'desc': 'CatÃ¡logo de productos con equivalencias',
        'agregar_ean': False
    },
    'ğŸ“‹ Stock AlmacÃ©n y Lote': {
        'sp': 'uspGC_ListarStockXAlmacenLoteMACROS',
        'params': [],
        'desc': 'Inventario por almacÃ©n y lote',
        'agregar_ean': False
    },
    'ğŸ’° Precio Lista': {
        'sp': 'uspGC_ObtenerPrecioListaMACROS',
        'params': [],
        'desc': 'Lista de precios activa',
        'agregar_ean': False
    },
    'ğŸ’µ Reporte Cartera': {
        'sp': 'usp_ReporteCarteraMACROS',
        'params': ['fecha_inicio', 'fecha_fin'],
        'desc': 'Estado de cartera clientes',
        'agregar_ean': False
    },
    'ğŸ“‘ Documento Vta Detallada': {
        'sp': 'uspGC_ListarDocumentoVtaDetalladaMACROS',
        'params': ['fecha_inicio', 'fecha_fin'],
        'desc': 'Documentos de venta detallados',
        'agregar_ean': True
    },
    'ğŸ’² Diferencia Precios': {
        'sp': 'uspGC_ListarDiferenciaPreciosMACROS',
        'params': ['fecha_inicio', 'fecha_fin'],
        'desc': 'AnÃ¡lisis diferencias de precios',
        'agregar_ean': False
    },
    'ğŸ“Š Fill Rate Cliente-Producto': {
        'sp': 'uspGC_ListarFillRateXClienteProductoMACROS',
        'params': ['fecha_inicio', 'fecha_fin'],
        'desc': 'Fill rate por cliente y producto',
        'agregar_ean': False
    }
}

# ==========================================================================
# FUNCIONES
# ==========================================================================

def get_connection(pais: str):
    """Establece conexiÃ³n con SQL Server (sin cache)"""
    try:
        config = SERVERS_CONFIG[pais]
        conn_str = (
            f'DRIVER={{{DRIVER}}};'
            f'SERVER={config["server"]};'
            f'DATABASE={config["database"]};'
            f'UID={config["user"]};'
            f'PWD={config["password"]};'
            f'TrustServerCertificate=yes;'
            f'Timeout=300;'
        )
        return pyodbc.connect(conn_str, timeout=30)
    except Exception as e:
        st.error(f"âŒ Error conectando a {pais}: {e}")
        return None


def desambiguar_columnas(df: pd.DataFrame) -> pd.DataFrame:
    """Renombra columnas duplicadas agregando sufijos _1, _2, etc."""
    if df.empty:
        return df
    
    columnas = list(df.columns)
    contadores = {}
    nuevas_columnas = []
    
    for col in columnas:
        if col in contadores:
            contadores[col] += 1
            nuevas_columnas.append(f"{col}_{contadores[col]}")
        else:
            contadores[col] = 0
            nuevas_columnas.append(col)
    
    # Si hubo duplicados, renombrar y notificar
    if any(c != nc for c, nc in zip(columnas, nuevas_columnas)):
        duplicados = [c for c in contadores if contadores[c] > 0]
        st.warning(f"âš ï¸ Columnas duplicadas renombradas: {', '.join(duplicados)}")
        df.columns = nuevas_columnas
    
    return df


def agregar_columna_ean(df: pd.DataFrame, pais: str) -> pd.DataFrame:
    """Agrega columna EAN mediante JOIN a maeGC_ProductoEquiv"""
    if df.empty:
        df['EAN'] = ''
        return df
    
    # Buscar columna de producto
    col_producto = None
    posibles = ['CÃ³digo de producto', 'CodigoProducto', 'cProducto', 
                'cProductoVta', 'ITEMNMBR', 'Codigo Producto']
    
    for nombre in posibles:
        if nombre in df.columns:
            col_producto = nombre
            break
    
    if not col_producto:
        df['EAN'] = ''
        st.warning(f"âš ï¸ {pais}: No se encontrÃ³ columna de producto para EAN")
        return df
    
    conn = get_connection(pais)
    if not conn:
        df['EAN'] = ''
        return df
    
    cursor = conn.cursor()
    
    try:
        codigo_pais = {'CHILE': 'CL', 'PERU': 'PE', 'COLOMBIA': 'CO', 'ECUADOR': 'EC'}.get(pais, 'CL')
        
        cursor.execute("SET NOCOUNT ON")
        cursor.execute("SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED")
        
        query_ean = f"""
        SELECT DISTINCT
            RTRIM(cProducto) AS cProducto,
            RTRIM(cProductoEquiv) AS EAN
        FROM dbo.maeGC_ProductoEquiv WITH (NOLOCK)
        WHERE cEquivalencia = 'EAN12' 
            AND cPais = '{codigo_pais}'
        OPTION (MAXDOP 4)
        """
        
        cursor.execute(query_ean)
        columns_ean = [col[0] for col in cursor.description]
        rows_ean = cursor.fetchall()
        
        if rows_ean and columns_ean:
            df_ean = pd.DataFrame.from_records(rows_ean, columns=columns_ean)
            
            df['_codigo_limpio'] = df[col_producto].astype(str).str.strip()
            df_ean['cProducto'] = df_ean['cProducto'].astype(str).str.strip()
            
            df = df.merge(
                df_ean[['cProducto', 'EAN']],
                left_on='_codigo_limpio',
                right_on='cProducto',
                how='left'
            )
            
            df.drop(columns=['_codigo_limpio', 'cProducto'], inplace=True, errors='ignore')
            df['EAN'] = df['EAN'].fillna('').astype(str)
            
            ean_count = (df['EAN'] != '').sum()
            st.success(f"âœ… {pais}: {ean_count:,} cÃ³digos EAN agregados ({ean_count/len(df)*100:.1f}%)")
        else:
            df['EAN'] = ''
            st.info(f"â„¹ï¸ {pais}: No se encontraron cÃ³digos EAN")
            
    except Exception as e:
        st.error(f"âŒ {pais}: Error agregando EAN: {e}")
        df['EAN'] = ''
    finally:
        cursor.close()
        conn.close()
    
    return df


def ejecutar_sp(pais: str, sp_name: str, params: list = None, agregar_ean: bool = False) -> pd.DataFrame:
    """Ejecuta stored procedure con optimizaciones"""
    conn = get_connection(pais)
    if not conn:
        return None
    
    cursor = conn.cursor()
    
    try:
        cursor.execute("SET NOCOUNT ON")
        cursor.execute("SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED")
        
        if params:
            placeholders = ', '.join(['?' for _ in params])
            query = f"EXEC {sp_name} {placeholders}"
            cursor.execute(query, params)
        else:
            cursor.execute(f"EXEC {sp_name}")
        
        columns = [col[0] for col in cursor.description] if cursor.description else []
        
        chunk_size = 10000
        all_rows = []
        while True:
            chunk = cursor.fetchmany(chunk_size)
            if not chunk:
                break
            all_rows.extend(chunk)
        
        if all_rows and columns:
            df = pd.DataFrame.from_records(all_rows, columns=columns)
            
            # Desambiguar columnas duplicadas
            df = desambiguar_columnas(df)
            
            if agregar_ean and 'EAN' not in df.columns:
                df = agregar_columna_ean(df, pais)
            
            return df
        return pd.DataFrame()
        
    except Exception as e:
        error_msg = str(e).lower()
        if "could not find stored procedure" in error_msg:
            st.warning(f"âš ï¸ {pais}: SP '{sp_name}' no existe")
        else:
            st.error(f"âŒ {pais}: Error ejecutando SP - {e}")
        return None
    finally:
        cursor.close()
        conn.close()


def detectar_columna_fecha(pais: str, tabla: str) -> str:
    """Detecta columna de fecha para filtro de 36 meses"""
    conn = get_connection(pais)
    if not conn:
        return None
    
    cursor = conn.cursor()
    
    try:
        query = f"""
        SELECT TOP 1 COLUMN_NAME
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_NAME = '{tabla}'
        AND DATA_TYPE IN ('datetime', 'date', 'smalldatetime', 'datetime2')
        ORDER BY ORDINAL_POSITION
        """
        cursor.execute(query)
        result = cursor.fetchone()
        return result[0] if result else None
    except:
        return None
    finally:
        cursor.close()
        conn.close()


def descargar_tabla(pais: str, tabla: str) -> pd.DataFrame:
    """Descarga tabla completa o filtrada por 36 meses (incluye EAN para movGC_vtDocumentoVtaDet)"""
    conn = get_connection(pais)
    if not conn:
        return None
    
    cursor = conn.cursor()
    
    try:
        columna_fecha = detectar_columna_fecha(pais, tabla)
        
        cursor.execute("SET NOCOUNT ON")
        cursor.execute("SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED")
        
        codigo_pais = {'CHILE': 'CL', 'PERU': 'PE', 'COLOMBIA': 'CO', 'ECUADOR': 'EC'}.get(pais, 'CL')
        
        # Query especial para movGC_vtDocumentoVtaDet con EAN
        if tabla.upper() == 'MOVGC_VTDOCUMENTOVTADET':
            if columna_fecha:
                fecha_inicio = (datetime.now() - timedelta(days=36*30)).strftime('%Y-%m-%d')
                query = f"""
                WITH EAN_LOOKUP AS (
                    SELECT 
                        RTRIM(cProducto) AS cProducto,
                        RTRIM(cProductoEquiv) AS EAN
                    FROM dbo.maeGC_ProductoEquiv WITH (NOLOCK)
                    WHERE cEquivalencia = 'EAN12' 
                        AND cPais = '{codigo_pais}'
                )
                SELECT 
                    d.*,
                    COALESCE(e.EAN, '') AS EAN
                FROM {tabla} d WITH (NOLOCK, INDEX(0))
                LEFT JOIN EAN_LOOKUP e ON RTRIM(d.cProductoVta) = e.cProducto
                WHERE d.{columna_fecha} >= '{fecha_inicio}'
                OPTION (MAXDOP 4, OPTIMIZE FOR UNKNOWN)
                """
            else:
                query = f"""
                WITH EAN_LOOKUP AS (
                    SELECT 
                        RTRIM(cProducto) AS cProducto,
                        RTRIM(cProductoEquiv) AS EAN
                    FROM dbo.maeGC_ProductoEquiv WITH (NOLOCK)
                    WHERE cEquivalencia = 'EAN12' 
                        AND cPais = '{codigo_pais}'
                )
                SELECT 
                    d.*,
                    COALESCE(e.EAN, '') AS EAN
                FROM {tabla} d WITH (NOLOCK, INDEX(0))
                LEFT JOIN EAN_LOOKUP e ON RTRIM(d.cProductoVta) = e.cProducto
                OPTION (MAXDOP 4)
                """
        elif columna_fecha:
            fecha_inicio = (datetime.now() - timedelta(days=36*30)).strftime('%Y-%m-%d')
            query = f"""
            SELECT * 
            FROM {tabla} WITH (NOLOCK, INDEX(0))
            WHERE {columna_fecha} >= '{fecha_inicio}'
            OPTION (MAXDOP 4, OPTIMIZE FOR UNKNOWN)
            """
        else:
            query = f"""
            SELECT * FROM {tabla} WITH (NOLOCK, INDEX(0))
            OPTION (MAXDOP 4)
            """
        
        cursor.execute(query)
        columns = [col[0] for col in cursor.description]
        
        chunk_size = 5000
        all_rows = []
        while True:
            chunk = cursor.fetchmany(chunk_size)
            if not chunk:
                break
            all_rows.extend(chunk)
        
        if all_rows and columns:
            df = pd.DataFrame.from_records(all_rows, columns=columns)
            
            # Desambiguar columnas duplicadas
            df = desambiguar_columnas(df)
            
            if tabla.upper() == 'MOVGC_VTDOCUMENTOVTADET' and 'EAN' in df.columns:
                ean_count = (df['EAN'] != '').sum()
                st.success(f"âœ… {pais}: Columna EAN incluida - {ean_count:,} de {len(df):,} ({ean_count/len(df)*100:.1f}%)")
            
            return df
        return pd.DataFrame()
        
    except Exception as e:
        st.error(f"âŒ {pais}: Error descargando tabla {tabla} - {e}")
        return None
    finally:
        cursor.close()
        conn.close()


def guardar_csv(df: pd.DataFrame, pais: str, nombre: str) -> str:
    """Guarda CSV en carpeta de Google Drive"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{nombre.replace(' ', '_')}_{timestamp}.csv"
    filepath = BASE_DIR / pais / filename
    
    df.to_csv(filepath, index=False, encoding='utf-8-sig')
    return str(filepath)


def mover_archivos_a_back(pais: str) -> int:
    """Mueve archivos CSV existentes a carpeta back antes de nueva descarga"""
    carpeta_pais = BASE_DIR / pais
    carpeta_back = carpeta_pais / "back"
    
    # Crear carpeta back si no existe
    carpeta_back.mkdir(parents=True, exist_ok=True)
    
    # Buscar archivos CSV en carpeta principal (no en subcarpetas)
    archivos_csv = list(carpeta_pais.glob("*.csv"))
    
    archivos_movidos = 0
    timestamp_back = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    for archivo in archivos_csv:
        # Agregar timestamp al nombre para evitar sobrescritura
        nombre_base = archivo.stem
        extension = archivo.suffix
        nuevo_nombre = f"{nombre_base}_bak_{timestamp_back}{extension}"
        destino = carpeta_back / nuevo_nombre
        
        try:
            archivo.rename(destino)
            archivos_movidos += 1
        except Exception as e:
            st.warning(f"âš ï¸ {pais}: No se pudo mover {archivo.name}: {e}")
    
    # Verificar que la carpeta principal quedÃ³ vacÃ­a de CSVs
    archivos_restantes = list(carpeta_pais.glob("*.csv"))
    if archivos_restantes:
        st.warning(f"âš ï¸ {pais}: {len(archivos_restantes)} archivo(s) no pudieron moverse")
    else:
        st.info(f"âœ… {pais}: Carpeta principal vacÃ­a y lista para nueva descarga")
    
    return archivos_movidos


# ==========================================================================
# INTERFAZ
# ==========================================================================

st.title("ğŸ“¥ Descarga SQL Server â†’ Google Drive")
st.markdown(f"**Destino:** `{DRIVE_BASE_DIR}`")
st.markdown("---")

# Sidebar
with st.sidebar:
    st.header("âš™ï¸ ConfiguraciÃ³n")
    
    paises_sel = st.multiselect(
        "ğŸŒ PaÃ­ses",
        options=PAISES,
        default=[PAISES[0]],
        help="Selecciona uno o mÃ¡s paÃ­ses"
    )
    
    if not paises_sel:
        st.warning("âš ï¸ Selecciona al menos un paÃ­s")
        st.stop()
    
    st.markdown("---")
    
    tipo_descarga = st.radio(
        "ğŸ“¥ Tipo de Descarga",
        options=["ğŸ”¹ Reportes (SP)", "ğŸ“Š Tablas Base (11)"],
        help="Reportes: SPs especÃ­ficos\nTablas: 11 tablas base para ETL"
    )
    
    st.markdown("---")
    
    if tipo_descarga == "ğŸ”¹ Reportes (SP)":
        reporte_sel = st.selectbox(
            "ğŸ“‹ Reporte",
            options=list(SPS_DISPONIBLES.keys())
        )
        
        config = SPS_DISPONIBLES[reporte_sel]
        st.info(config['desc'])
        
        with st.expander("ğŸ”§ Detalles"):
            st.code(f"SP: {config['sp']}")
            if config.get('agregar_ean'):
                st.success("âœ… Incluye columna EAN automÃ¡tica")
    else:
        st.info("ğŸ“Š Descarga de 11 tablas base para ETL")
        
        with st.expander("ğŸ“‹ Tablas (11)"):
            for tabla in TABLAS_BASE:
                if tabla == "movGC_vtDocumentoVtaDet":
                    st.text(f"âœ… {tabla} (con EAN)")
                else:
                    st.text(f"â€¢ {tabla}")
        
        st.warning("âš ï¸ Esta descarga puede tardar varios minutos")

# Ãrea principal
if tipo_descarga == "ğŸ”¹ Reportes (SP)":
    st.header(reporte_sel)
    
    params = []
    if 'fecha_inicio' in config['params']:
        col1, col2 = st.columns(2)
        with col1:
            fecha_ini = st.date_input(
                "ğŸ“… Fecha Inicio",
                value=datetime.now().replace(day=1)
            )
            params.append(fecha_ini.strftime('%Y-%m-%d'))
        with col2:
            fecha_fin = st.date_input(
                "ğŸ“… Fecha Fin",
                value=datetime.now()
            )
            params.append(fecha_fin.strftime('%Y-%m-%d'))
    
    st.markdown("---")
    boton_texto = "â–¶ï¸ Ejecutar Reporte"
else:
    st.header("ğŸ“Š Descarga de Tablas Base")
    st.markdown("**Filtro:** Ãšltimos 36 meses (donde aplique)")
    st.markdown("**EAN:** Incluido automÃ¡ticamente en `movGC_vtDocumentoVtaDet`")
    st.markdown("---")
    boton_texto = "â–¶ï¸ Descargar 11 Tablas"

# EjecuciÃ³n
if st.button(boton_texto, type="primary", use_container_width=True):
    # Mover archivos existentes a back
    st.subheader("ğŸ“¦ Preparando Descarga")
    with st.expander("ğŸ—‚ï¸ Respaldo de archivos existentes", expanded=False):
        total_movidos = 0
        for pais in paises_sel:
            movidos = mover_archivos_a_back(pais)
            if movidos > 0:
                st.success(f"âœ… {pais}: {movidos} archivo(s) movido(s) a /back")
                total_movidos += movidos
            else:
                st.info(f"â„¹ï¸ {pais}: Sin archivos previos")
        
        if total_movidos > 0:
            st.success(f"âœ… Total: {total_movidos} archivo(s) respaldado(s)")
    
    st.markdown("---")
    st.subheader("ğŸ“Š Resultados")
    
    resultados = []
    
    if tipo_descarga == "ğŸ”¹ Reportes (SP)":
        # MODO REPORTES
        progress = st.progress(0)
        status = st.empty()
        
        for idx, pais in enumerate(paises_sel):
            status.text(f"Procesando {pais}...")
            
            agregar_ean = config.get('agregar_ean', False)
            df = ejecutar_sp(pais, config['sp'], params if params else None, agregar_ean=agregar_ean)
            
            if df is not None and not df.empty:
                filepath = guardar_csv(df, pais, reporte_sel)
                
                resultados.append({
                    'PaÃ­s': pais,
                    'Tipo': 'Reporte',
                    'Nombre': reporte_sel,
                    'Registros': len(df),
                    'Columnas': len(df.columns),
                    'Archivo': Path(filepath).name
                })
                
                with st.expander(f"ğŸŒ {pais} - {len(df):,} registros", expanded=False):
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("ğŸ“Š Registros", f"{len(df):,}")
                    with col2:
                        st.metric("ğŸ“‹ Columnas", len(df.columns))
                    with col3:
                        st.metric("ğŸ’¾ TamaÃ±o", f"{df.memory_usage(deep=True).sum() / 1024:.1f} KB")
                    
                    st.dataframe(df.head(50), use_container_width=True)
                    st.success(f"âœ… `{filepath}`")
            
            elif df is not None:
                st.warning(f"âš ï¸ {pais}: Sin datos")
            
            progress.progress((idx + 1) / len(paises_sel))
        
        status.text("âœ… Completado")
    
    else:
        # MODO TABLAS BASE
        total_ops = len(paises_sel) * len(TABLAS_BASE)
        progress = st.progress(0)
        status = st.empty()
        op_actual = 0
        
        for pais in paises_sel:
            st.markdown(f"### ğŸŒ {pais}")
            
            for tabla in TABLAS_BASE:
                op_actual += 1
                status.text(f"Descargando {tabla} de {pais}... ({op_actual}/{total_ops})")
                
                df = descargar_tabla(pais, tabla)
                
                if df is not None and not df.empty:
                    filepath = guardar_csv(df, pais, tabla)
                    
                    resultados.append({
                        'PaÃ­s': pais,
                        'Tipo': 'Tabla',
                        'Nombre': tabla,
                        'Registros': len(df),
                        'Columnas': len(df.columns),
                        'Archivo': Path(filepath).name
                    })
                    
                    st.success(f"âœ… {tabla}: {len(df):,} registros, {len(df.columns)} columnas")
                
                elif df is not None:
                    st.warning(f"âš ï¸ {tabla}: Sin datos")
                else:
                    st.error(f"âŒ {tabla}: Error")
                
                progress.progress(op_actual / total_ops)
        
        status.text("âœ… Descarga completada")
    
    # Resumen
    if resultados:
        st.markdown("---")
        st.subheader("ğŸ“ Resumen de Descarga")
        
        df_res = pd.DataFrame(resultados)
        st.dataframe(df_res, use_container_width=True, hide_index=True)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ğŸŒ PaÃ­ses", len(paises_sel))
        with col2:
            st.metric("ğŸ“Š Total Registros", f"{df_res['Registros'].sum():,}")
        with col3:
            st.metric("ğŸ“ Archivos", len(resultados))
        
        st.info(f"ğŸ’¡ **PrÃ³ximos pasos:** Ejecuta `python pipeline_maestro.py` para procesar y cargar a Snowflake")
    else:
        st.warning("âš ï¸ No se obtuvieron resultados")

# Footer
st.markdown("---")
st.caption(f"Pipeline ETL - Paso 1: Descarga SQL Server | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
