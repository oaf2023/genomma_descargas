# üìã Pipeline ETL - Google Drive Desktop + Snowflake

**Versi√≥n:** 1.0  
**Fecha:** 2026-01-22  
**Ubicaci√≥n:** `C:\Ciencia de Datos\Proceso_Snowflake`

---

## üéØ Objetivo

Pipeline ETL completo para procesar datos de SQL Server ‚Üí Google Drive Desktop ‚Üí Snowflake, con visualizaci√≥n en Streamlit.

**Pa√≠ses soportados:** Chile, Colombia, Ecuador, Per√∫

---

## üìÅ Estructura del Proyecto

```
Proceso_Snowflake/
‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îú‚îÄ‚îÄ .env.template          # Plantilla de configuraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ .env                   # TU configuraci√≥n (gitignore)
‚îÇ   ‚îú‚îÄ‚îÄ 2_normalizar_headers.py
‚îÇ   ‚îú‚îÄ‚îÄ 3_renombrar_archivos.py
‚îÇ   ‚îî‚îÄ‚îÄ 4_cargar_snowflake.py
‚îú‚îÄ‚îÄ streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ app_reportes.py        # Dashboard Snowflake
‚îú‚îÄ‚îÄ logs/                       # Logs de ejecuci√≥n
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ README.md              # Este archivo
```

---

## üöÄ Setup Inicial

### 1. Instalar Google Drive Desktop

1. Descarga: https://www.google.com/drive/download/
2. Instala y configura tu cuenta Google
3. Crea carpeta en Drive: `ETL_Snowflake/`
4. Dentro crea: `CHILE/`, `COLOMBIA/`, `ECUADOR/`, `PERU/`
5. Anota la ruta local (ejemplo: `G:\Mi unidad\ETL_Snowflake`)

### 2. Configurar Variables de Entorno

```powershell
cd "C:\Ciencia de Datos\Proceso_Snowflake\etl"

# Copiar plantilla
Copy-Item .env.template .env

# Editar con tus credenciales
notepad .env
```

**Variables obligatorias:**
```dotenv
DRIVE_BASE_DIR=G:\Mi unidad\ETL_Snowflake
PAISES_FOLDERS=CHILE,COLOMBIA,ECUADOR,PERU

SNOWFLAKE_ACCOUNT=tu_account.region
SNOWFLAKE_USER=tu_usuario
SNOWFLAKE_PASSWORD=tu_password
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_DATABASE=DEV_LND
SNOWFLAKE_SCHEMA=_SQL_CHI
SNOWFLAKE_ROLE=tu_role
```

### 3. Instalar Dependencias

```powershell
cd "C:\Ciencia de Datos\Proceso_Snowflake"

# Crear entorno virtual (opcional pero recomendado)
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Instalar paquetes
pip install polars pandas pyarrow snowflake-connector-python python-dotenv streamlit
```

---

## üìä Pipeline - Flujo Completo

### Paso 1: Descargar desde SQL Server (Streamlit)

**Herramienta:** `1_descargar_sql_server.py`

```powershell
cd "C:\Ciencia de Datos\Proceso_Snowflake"
streamlit run etl/1_descargar_sql_server.py
```

**11 Reportes Disponibles:**
1. üìä **Reporte √önico de Ventas** (con EAN autom√°tico)
2. üìà **Reporte Ventas Sellin** (con EAN autom√°tico)
3. üè™ **Reporte Ventas Mercado** (con EAN autom√°tico)
4. üë• **Listar Clientes**
5. üì¶ **Listar Productos Detallado**
6. üìã **Stock Almac√©n y Lote**
7. üí∞ **Precio Lista**
8. üíµ **Reporte Cartera**
9. üìë **Documento Vta Detallada** (con EAN autom√°tico)
10. üí≤ **Diferencia Precios**
11. üìä **Fill Rate Cliente-Producto**

**Columna EAN:**
- ‚úÖ Se agrega autom√°ticamente en reportes de ventas (1, 2, 3, 9)
- ‚úÖ JOIN directo a `maeGC_ProductoEquiv` durante la descarga
- ‚úÖ Sin necesidad de pasos adicionales

**Acciones:**
1. Selecciona pa√≠s(es)
2. Selecciona reporte
3. Ingresa fechas (si aplica)
4. Ejecuta ‚Üí Guarda autom√°ticamente en Drive: `G:\Mi unidad\ETL_Snowflake\{PAIS}\`

### Paso 2: Normalizar Headers CSV

**Script:** `2_normalizar_headers.py`

```powershell
cd "C:\Ciencia de Datos\Proceso_Snowflake\etl"

# Dry-run (ver cambios sin aplicar)
python 2_normalizar_headers.py --dry-run

# Ejecutar normalizaci√≥n
python 2_normalizar_headers.py
```

**Transformaciones:**
- `Cod. Cliente` ‚Üí `COD_CLIENTE`
- `Raz√≥n Social` ‚Üí `RAZON_SOCIAL`
- `N√∫mero Cliente` ‚Üí `NUMERO_CLIENTE`
- Etc. (ver mapeo completo en script)

**Salida:** Archivos `*_normalizado.csv` en cada carpeta pa√≠s

### Paso 3: Renombrar Archivos

**Script:** `3_renombrar_archivos.py`

```powershell
# Dry-run (ver cambios)
python 3_renombrar_archivos.py

# Ejecutar renombrado
python 3_renombrar_archivos.py --apply
```

**Transformaciones:**
- Remueve timestamps: `_20260122_143025`
- Garantiza sufijo pa√≠s: `_CHILE_normalizado.csv`
- Limpia guiones bajos m√∫ltiples

### Paso 4: Cargar a Snowflake

**Script:** `4_cargar_snowflake.py`

```powershell
python 4_cargar_snowflake.py
```

**Proceso:**
1. Detecta separador CSV autom√°ticamente (`;` o `,`)
2. Lee con Pandas/Polars
3. **Crea backup:** `{TABLA}` ‚Üí `{TABLA}_OLD`
4. Crea tabla nueva con estructura del CSV
5. Convierte a Parquet
6. Carga con `PUT` + `COPY INTO`

**IMPORTANTE:**
- ‚úÖ Backups autom√°ticos protegen datos existentes
- ‚úÖ Columna EAN garantizada en la carga
- ‚úÖ Nombres de tabla sin acentos (√önico ‚Üí UNICO)

**Logs:** Guardados en `Proceso_Snowflake/logs/`

---

## üìä Streamlit - Dashboard

### Ejecutar Localmente

```powershell
cd "C:\Ciencia de Datos\Proceso_Snowflake"
streamlit run streamlit/app_reportes.py
```

Abre: http://localhost:8501

### Desplegar en Streamlit Cloud

1. Sube proyecto a GitHub (excluir `.env`)
2. En https://share.streamlit.io ‚Üí "New app"
3. Selecciona repo y archivo: `streamlit/app_reportes.py`
4. Configura secrets en Settings ‚Üí Secrets:

```toml
[snowflake]
account = "tu_account.region"
user = "tu_usuario"
password = "tu_password"
warehouse = "COMPUTE_WH"
database = "DEV_LND"
schema = "_SQL_CHI"
role = "tu_role"
```

5. Deploy

### Desplegar en Streamlit in Snowflake

```sql
-- En Snowflake Worksheet
USE DATABASE DEV_LND;
USE SCHEMA _SQL_CHI;

-- Crear Streamlit app
CREATE STREAMLIT APP_REPORTES_MULTIPAIS
  FROM '@~/streamlit'
  MAIN_FILE = 'app_reportes.py';

-- Subir archivo
PUT file://C:\Ciencia de Datos\Proceso_Snowflake\streamlit\app_reportes.py @~/streamlit/;

-- Ejecutar
ALTER STREAMLIT APP_REPORTES_MULTIPAIS SET COMMENT = 'Dashboard Multi-Pa√≠s';
```

---

## üîß Comandos √ötiles

### Verificar Estado de Tablas en Snowflake

```sql
-- Ver todas las tablas (incluyendo backups)
SHOW TABLES IN SCHEMA _SQL_CHI;

-- Ver solo tablas activas (sin _OLD)
SHOW TABLES LIKE '%CHILE' IN SCHEMA _SQL_CHI;

-- Verificar columnas de una tabla
DESC TABLE REPORTE_UNICO_DE_VENTAS_CHILE;

-- Ver backups
SHOW TABLES LIKE '%_OLD' IN SCHEMA _SQL_CHI;
```

### Restaurar desde Backup

```sql
-- Si necesitas restaurar tabla desde _OLD
DROP TABLE REPORTE_UNICO_DE_VENTAS_CHILE;
ALTER TABLE REPORTE_UNICO_DE_VENTAS_CHILE_OLD 
  RENAME TO REPORTE_UNICO_DE_VENTAS_CHILE;
```

### Limpiar Backups Antiguos

```sql
-- Eliminar todas las tablas _OLD
DROP TABLE IF EXISTS REPORTE_UNICO_DE_VENTAS_CHILE_OLD;
DROP TABLE IF EXISTS LISTAR_CLIENTES_CHILE_OLD;
-- ... etc
```

---

## üêõ Troubleshooting

### Error: "Google Drive no detectado"

**Causa:** Ruta incorrecta o Drive Desktop no instalado

**Soluci√≥n:**
```powershell
# Verificar ruta
Test-Path "G:\Mi unidad\ETL_Snowflake"

# Actualizar .env con ruta correcta
notepad etl\.env
```

### Error: "Columna EAN no se carga"

**Causa:** CSV mal formado o separador incorrecto

**Soluci√≥n:**
- ‚úÖ Script detecta separador autom√°ticamente
- ‚úÖ Usa Pandas con `on_bad_lines='skip'`
- ‚úÖ Backup autom√°tico protege tabla existente

### Error: "Tabla con acentos"

**Causa:** Nombre de archivo con caracteres especiales

**Soluci√≥n:**
- ‚úÖ Script normaliza autom√°ticamente: √önico ‚Üí UNICO
- ‚úÖ Remueve emojis y s√≠mbolos

### Error de conexi√≥n Snowflake

**Causa:** Credenciales incorrectas o red

**Soluci√≥n:**
```powershell
# Verificar credenciales
python -c "from dotenv import load_dotenv; import os; load_dotenv('etl/.env'); print(os.getenv('SNOWFLAKE_ACCOUNT'))"

# Probar conexi√≥n
python -c "import snowflake.connector; snowflake.connector.connect(...)"
```

---

## üìà Mejoras Futuras

- [ ] Automatizar descarga SQL Server ‚Üí Drive (script programado)
- [ ] Alertas por email en caso de errores
- [ ] Dashboard con m√©tricas de calidad de datos
- [ ] Integraci√≥n con Apache Airflow para orquestaci√≥n
- [ ] Migraci√≥n a Google Cloud Storage (GCS) para External Stages nativos

---

## üìû Soporte

**Ubicaci√≥n c√≥digo:** `C:\Ciencia de Datos\Proceso_Snowflake`  
**Logs:** `Proceso_Snowflake/logs/`  
**Documentaci√≥n adicional:** `C:\Ciencia de Datos\AGENTS.MD`

---

## üîê Seguridad

‚ö†Ô∏è **NUNCA commitear `.env` a Git**

Agregar a `.gitignore`:
```gitignore
.env
*.log
logs/
.venv/
__pycache__/
```

---

## üìù Changelog

**v1.0 (2026-01-22)**
- ‚úÖ Configuraci√≥n Google Drive Desktop
- ‚úÖ Pipeline ETL completo (4 pasos)
- ‚úÖ Backups autom√°ticos (_OLD)
- ‚úÖ Detecci√≥n autom√°tica de separador CSV
- ‚úÖ Dashboard Streamlit multi-pa√≠s
- ‚úÖ Documentaci√≥n completa

---

*√öltima actualizaci√≥n: 2026-01-22*
